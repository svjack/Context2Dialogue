<!-- PROJECT LOGO -->
<br />
<p align="center">
  <h3 align="center">Context2Dialogue</h3>

  <p align="center">
   		A Dialogue Context Generator based on Text or Image
    <br />
  </p>
</p>

[ä¸­æ–‡ç®€ä»‹](README.md)

## Introduction
### Project view in the point of dialogue goal
Given the first sentence of a dialogue, one can get the subsequent context by
a seq2seq style model. I have released two repositories called [svjack/Daliy-Dialogue](https://github.com/svjack/Daliy-Dialogue) and [svjack/GLM-Open-Dialogue](https://github.com/svjack/GLM-Open-Dialogue) do this kinds of works. </br>

The difference between them is, the former is simply a GPT or Bloom seq2seq style model that predict the subsequent without knowledge's help or restraint, the latter use General Language Model (GLM) do auxiliary and do some reconstructions. </br>

This project focus on achieve the goal like [deepset](https://huggingface.co/deepset)'s [wikipedia-assistant](https://huggingface.co/spaces/deepset/wikipedia-assistant) in Dialogue domain. [wikipedia-assistant](https://huggingface.co/spaces/deepset/wikipedia-assistant) use a seq2seq model to generate long form answer based on wikipedia context (recall by a faiss index). The answers' validity is guaranteed by the context, which makes them more closed to reality.</br>

In the  [svjack/Daliy-Dialogue](https://github.com/svjack/Daliy-Dialogue) situation, no reality guarantee, In the [svjack/GLM-Open-Dialogue](https://github.com/svjack/GLM-Open-Dialogue) situation, the reality guarantee is rely on pretrained General Language Model (GLM). This project more close to the fact compared with above two
 repositories and have a relative rapid running performance (speed) than  [svjack/GLM-Open-Dialogue](https://github.com/svjack/GLM-Open-Dialogue).</br>
 
Try and compare [svjack/context-dialogue-chinese-sample-search](https://huggingface.co/spaces/svjack/context-dialogue-chinese-sample-search) and [svjack/bloom-gpt-dialogue-chinese-sample-search](https://huggingface.co/spaces/svjack/bloom-gpt-dialogue-chinese-sample-search)
with similar questions as input, you will understand the difference between this project and first sentence guide dialogue prediction models in Chinese domain.

### Project view in the point of generation
[svjack/docvqa-gen](https://github.com/svjack/docvqa-gen) is a project that One can use text or Document Image as Input and get question-answer pairs as Output. In the point of generation is a â€œcontext to qaâ€ generator. This project is the counterpart in â€œcontext to dialogueâ€ scene.


## HuggingFace demonstration

### Used Related Model demonstration (Name startswith svjack is trained by myself)
|Name |HuggingFace Model link| HuggingFace Space link | Language | Model skeleton |
|---------|--------|-------|-------|------|
| svjack/summary-dialogue-eng | https://huggingface.co/svjack/summary-dialogue-eng | https://huggingface.co/spaces/svjack/English-Context-Dialogue-Generator | English | T5 |
| svjack/summary-dialogue | https://huggingface.co/svjack/summary-dialogue | https://huggingface.co/spaces/svjack/Chinese-Context-Dialogue-Generator | Chinese | T5 |
| daspartho/prompt-extend | https://huggingface.co/daspartho/prompt-extend | https://huggingface.co/spaces/daspartho/prompt-extend | English | GPT2 |
| svjack/prompt-extend-chinese-gpt | https://huggingface.co/svjack/prompt-extend-chinese-gpt | https://huggingface.co/spaces/svjack/prompt-extend-gpt-chinese | Chinese | GPT2 |
| nlpconnect/vit-gpt2-image-captioning | https://huggingface.co/nlpconnect/vit-gpt2-image-captioning |https://huggingface.co/spaces/SRDdev/Image-Caption | English | VIT X GPT2 |
| YeungNLP/ofa-cn-base-muge-v2 |https://huggingface.co/YeungNLP/ofa-cn-base-muge-v2 ||Chinese| OFA |

<br/>
Try to select the "do_sample" option when you do not get the satisfying output when try https://huggingface.co/spaces/svjack/English-Context-Dialogue-Generator and https://huggingface.co/spaces/svjack/Chinese-Context-Dialogue-Generator
You may have a try to wrap this option by a reinforce style reward extention in a reinforcement learning manner to generate samples.

### Dataset generate by above models demonstration
|Name |HuggingFace Dataset link| HuggingFace Space link | Language |
|---------|--------|-------|-------|
| svjack/context-dialogue-generate-ds-zh-v1 | https://huggingface.co/datasets/svjack/context-dialogue-generate-ds-zh-v1 | https://huggingface.co/spaces/svjack/context-dialogue-chinese-sample-search | Chinese |

## Installation and Instructions

Refer to HuggingFace Model cards.

### Installation
```bash
pip install -r requirements.txt
```
### Instructions
#### simply call one times

* 1 English Dialogue Generator in Text ğŸ¦…:

```python
from summary_reverse_pred_eng_native import *

en_context = "The Wisconsin Territorial Centennial half dollar was designed by David Parsons and Benjamin Hawkins and minted by the United States Bureau of the Mint in 1936. The obverse (pictured) depicts a pick axe and lead ore, referring to the lead mining in early Wisconsin"

simple_pred(en_context, do_sample = False)
```

will output:
```json
['Have you seen the Wisconsin Territorial Centennial half dollar?',
 'Yeah, it was designed by David Parsons and Benjamin Hawkins.',
 'What is it?',
 "It's a half dollar with a pick axe and lead ore.",
 "That's great!"]
```

</br>

* 2 Chinese Dialogue Generator in Text ğŸ°:

```python
from summary_reverse_pred_native import *

zh_context = "å·´ä¼åˆ©äºšå·æˆ˜åˆ—èˆ°[a]ï¼ˆå¾·è¯­ï¼šSMS Bayern[b]ï¼‰æ˜¯å¾·æ„å¿—å¸å›½æµ·å†›å·´ä¼åˆ©äºšçº§æˆ˜åˆ—èˆ°çš„ä¸»å¯¼èˆ°ã€‚è¯¥èˆ°äº1915å¹´2æœˆä¸‹æ°´å¹¶äº1916å¹´7æœˆå¼€å§‹æœå½¹ï¼Œä½†å·²æ¥ä¸åŠå‚åŠ æ—¥å¾·å…°æµ·æˆ˜ã€‚å®ƒçš„ä¸»ç‚®åŒ…æ‹¬åˆ†å¸ƒåœ¨å››åº§åŒè”è£…ç‚®å¡”ä¸­çš„å…«é—¨380æ¯«ç±³å£å¾„ç‚®ï¼Œè¿™æ¯”å…¶å‰èº«å›½ç‹çº§é…å¤‡çš„åé—¨305æ¯«ç±³å£å¾„ç‚®æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚[c]èˆ°åªè¿åŒå®ƒçš„ä¸‰è‰˜å§Šå¦¹èˆ°å·²ç»å½¢æˆäº†å…¬æµ·èˆ°é˜Ÿç¬¬å››æˆ˜åˆ—åˆ†èˆ°é˜Ÿçš„æ ¸å¿ƒã€‚è€Œè¿™å½“ä¸­ä»…æœ‰ä¸€è‰˜ï¼Œå³å·´ç™»å·å®Œæˆå»ºé€ ï¼›å¦å¤–ä¸¤è‰˜åˆ™åœ¨ç¬¬ä¸€æ¬¡ä¸–ç•Œå¤§æˆ˜åæœŸï¼Œå½“ç”Ÿäº§éœ€æ±‚è¢«è½¬ç§»è‡³Uå‹æ½œè‰‡åè€Œæ’¤é”€ã€‚"

simple_pred(zh_context, do_sample = False)
```

will output:
```json
['æ°å…‹:å·´ç½—åˆ©äºšå·æˆ˜åˆ—èˆ°æ˜¯å“ªä¸ªå›½å®¶?',
 'å®‰å¨œ:å¾·æ„å¿—å¸å›½æµ·å†›çš„ã€‚å®ƒåœ¨1915å¹´2æœˆä¸‹æ°´,19167å¼€å§‹æœå½¹',
 'æ°å…‹:è¯¥èˆ°çš„ä¸»è¦è£…å¤‡æ˜¯ä»€ä¹ˆ?',
 'å®‰å¨œ:ä¸»ç‚®åŒ…æ‹¬å››åº§åŒè”è£…ç‚®å¡”ä¸­çš„å…«é—¨380æ¯«ç±³å£å¾„ã€‚',
 'æ°å…‹:è¿™æ¯”å…¶å‰èº«å›½ç‹çº§è£…å¤‡çš„åé—¨305æ¯«ç±³å£å¾„ç‚®æœ‰äº†æ˜æ˜¾æ”¹è¿›ã€‚',
 'å®‰å¨œ:ä½†åªæœ‰ä¸‰è‰˜å§Šå¦¹èˆ°å·²ç»å½¢æˆå…¬æµ·èˆ°é˜Ÿç¬¬å››æˆ˜åˆ—åˆ†çš„æ ¸å¿ƒã€‚',
 'æ°å…‹:è¿™æ˜¯ä¸ºä»€ä¹ˆ?',
 'å®‰å¨œ:å®ƒæ˜¯äºŒæˆ˜åå»ºé€ çš„ã€‚']
```

</br>

#### some bad cases and remedy

```python
from summary_reverse_pred_eng_native import *

en_context = "Cyclone Gabrielle causes widespread damage and flooding across New Zealand."

simple_pred(en_context, do_sample = False)
```

will output:
```json
['file_photo>',
 'Cyclone Gabrielle!',
 "What's that?",
 "It's massive damage and flooding across New Zealand."]
```

</br>

```python
from summary_reverse_pred_native import *

zh_context = 'é•¿å®—æˆ‘éƒ¨æœ¬é˜Ÿåšå®ˆé•¿æ¿‘å·ï¼Œå½“è—¤å ‚å†›æ¥è¿‘æ—¶ï¼Œé•¿å®—æˆ‘éƒ¨é˜Ÿç«‹å³å‘½ä»¤éƒ¨é˜Ÿä»¥é“ç‚®å°„å‡»[47]ï¼Œç„¶åè¿›å…¥æ··æˆ˜ï¼Œç„¶åé•¿å®—æˆ‘éƒ¨è¿›è¡Œçªå‡»ï¼Œè—¤å ‚é«˜åˆ‘ã€è—¤å ‚æ°èƒœå’Œæ¡‘åä¸€å­æˆ˜æ­»[46]ï¼Œè—¤å ‚é˜Ÿå…ˆé”‹è¢«æ¶ˆç­ï¼Œé•¿å®—æˆ‘éƒ¨ä¸‰å†›è¿›è¡ŒæŒŸå‡»æœ¬é˜Ÿ'

simple_pred(zh_context, do_sample = False)
```

will output:
```json
['æ°å…‹:é•¿å®—æˆ‘éƒ¨æœ¬é˜Ÿåšå®ˆé•¿æ¿‘å·,å½“è—¤å ‚å†›æ¥è¿‘æ—¶,éƒ¨é˜Ÿç«‹å³å‘½ä»¤ä»¥é“ç‚®å°„å‡»[47],ç„¶åè¿›å…¥æ··æˆ˜,è¿›è¡Œçªå‡»,è—¤å ‚é«˜åˆ‘ã€æ°èƒœå’Œæ¡‘åä¸€å­æˆ˜æ­»,è—¤å ‚é˜Ÿå…ˆé”‹è¢«æ¶ˆç­,ä¸‰å†›æŒŸå‡»ã€‚']
```
</br>
</br>
The above two cases show that some sentence will yield dialogue predictions with few rounds. A simple but valid remedy of this problem is set "do_sample" parameter to True and try multiple times.
</br>
</br>
Below is the remedy demonstration.

```python
from summary_reverse_pred_eng_native import *

en_context = "Cyclone Gabrielle causes widespread damage and flooding across New Zealand."

df = sample_pred_wrapper(en_context, i2c_obj = i2c_obj)
df["dialogue"].values.tolist()
```

will output:
```json
[['What is the weather like in New Zealand?',
  'Cyclone Gabrielle, a massive flooding and hailstorm.',
  "I'm not sure what to make of it",
  'You can talk about it',
  "It's really terrible!"],
 ["What's the weather like in New Zealand?",
  'Cyclone Gabrielle is causing massive damage and flooding.',
  "I guess it's not that bad, but it can be worse than other parts of the country.",
  'But we have to go with a bit of care.'],
 ['Cyclone Gabrielle has broken through New Zealand!',
  "It's a disaster!",
  "I hope it won't be so bad.",
  "But it's really terrible!"],
 ['file_photo>',
  "What's this?",
  'Cyclone Gabrielle!',
  'Where are you?',
  'New Zealand, I think',
  'And what happened?',
  "It's very bad. The roads are so muddy",
  "Why doesn't it rain?",
  'We have to go through these terrible floods',
  "So we're going to be forced to stay in the city"],
 ['How is the weather today?',
  "It's ok, but it's not good",
  'What happened?',
  'Cyclone Gabrielle has spread across New Zealand',
  "I hope it won't be too bad",
  'The worst is already behind us',
  'We need to take care of ourselves']]
```

</br>

```python
from summary_reverse_pred_native import *

zh_context = 'é•¿å®—æˆ‘éƒ¨æœ¬é˜Ÿåšå®ˆé•¿æ¿‘å·ï¼Œå½“è—¤å ‚å†›æ¥è¿‘æ—¶ï¼Œé•¿å®—æˆ‘éƒ¨é˜Ÿç«‹å³å‘½ä»¤éƒ¨é˜Ÿä»¥é“ç‚®å°„å‡»[47]ï¼Œç„¶åè¿›å…¥æ··æˆ˜ï¼Œç„¶åé•¿å®—æˆ‘éƒ¨è¿›è¡Œçªå‡»ï¼Œè—¤å ‚é«˜åˆ‘ã€è—¤å ‚æ°èƒœå’Œæ¡‘åä¸€å­æˆ˜æ­»[46]ï¼Œè—¤å ‚é˜Ÿå…ˆé”‹è¢«æ¶ˆç­ï¼Œé•¿å®—æˆ‘éƒ¨ä¸‰å†›è¿›è¡ŒæŒŸå‡»æœ¬é˜Ÿ'

df = sample_pred_wrapper(zh_context, i2c_obj = ofa_obj)
df["dialogue"].values.tolist()
```

will output:
```json
[['æ°å…‹:é•¿å®—æˆ‘éƒ¨æœ¬é˜Ÿ,ä½ ä»¬åœ¨å“ªé‡Œ?',
  'å®‰å¨œ:æˆ‘ä»¬åšå®ˆé•¿æ¿‘å·,å½“è—¤å ‚å†›æ¥è¿‘æ—¶,ä»–ä»¬ç«‹å³å‘½ä»¤éƒ¨é˜Ÿä»¥é“ç‚®å°„å‡»[47],ç„¶åè¿›å…¥æ··æˆ˜,è¿›è¡Œçªå‡»,è—¤å ‚é«˜åˆ‘ã€æ°èƒœå’Œæ¡‘åä¸€å­æˆ˜æ­»ã€‚',
  'æ°å…‹:å“¦,å¤©å“ª,è¿™å¤ªå¯æ€•äº†!',
  'å®‰å¨œ:è¿˜æœ‰äººåœ¨æˆ˜æ–—ä¸­è¢«åˆ‡æ–­äº†çº¿ã€‚ é•¿å®—æˆ‘éƒ¨ä¸‰å†›è¿›è¡ŒæŒŸå‡»æœ¬é˜Ÿ'],
 ['æ°å…‹:é•¿å®—æˆ‘éƒ¨æœ¬é˜Ÿåœ¨å“ªé‡Œ?',
  'å®‰å¨œ:åœ¨é•¿æ¿‘å·,å½“è—¤å ‚å†›æ¥è¿‘æ—¶,æˆ‘çš„éƒ¨é˜Ÿç«‹å³å‘½ä»¤ä»–ä»¬ä»¥é“ç‚®å°„å‡»[47]ã€‚ç„¶åè¿›å…¥æ··æˆ˜,è¿›è¡Œçªå‡»',
  'æ°å…‹:è¿™æ˜¯ä»€ä¹ˆ? é•¿å®—æˆ‘éƒ¨:è—¤å ‚é«˜åˆ‘ã€æ°èƒœå’Œæ¡‘åä¸€å­æˆ˜æ­»ã€‚',
  'å®‰å¨œ:å¤§å± æ€æ˜¯ä»€ä¹ˆæ—¶å€™å¼€å§‹çš„? é•¿å®—æˆ‘éƒ¨ä¸‰å†›æ­£åœ¨è¿›è¡ŒæŒŸå‡»æœ¬é˜Ÿã€‚'],
 ['æ°å…‹:é•¿å®—æˆ‘éƒ¨ä¸‰å†›å›´æ”»æœ¬é˜Ÿ',
  'å®‰å¨œ:ä¸ºä»€ä¹ˆ?',
  'æ°å…‹:æˆ‘ä»¬åšå®ˆé•¿æ¿‘å·,å½“è—¤å ‚å†›æ¥è¿‘æ—¶,ä»–ä»¬ç«‹å³å‘½ä»¤éƒ¨é˜Ÿä»¥é“ç‚®å°„å‡»[47]ã€‚ç„¶åè¿›å…¥æ··æˆ˜,è¿›è¡Œçªå‡»,è—¤å ‚é«˜åˆ‘ã€æ°èƒœå’Œæ¡‘åä¸€å­æˆ˜æ­»',
  'å®‰å¨œ:å“¦,å¤©å“ª,è¿™å¾ˆç³Ÿç³•!',
  'æ°å…‹:å¤§å± æ€ä¹‹å,è—¤å ‚é˜Ÿé•¿è¢«è§£æ•£äº†ã€‚ é•¿å®—æˆ‘éƒ¨æœ¬é˜Ÿ:è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ ä¸€ç›´èº²åœ¨é•¿æ¿‘å·'],
 ['æ°å…‹:é•¿å®—æˆ‘éƒ¨æœ¬é˜Ÿåšå®ˆé•¿æ¿‘å·,å½“è—¤å ‚å†›æ¥è¿‘æ—¶,ä»–ä»¬å°†ç«‹å³å‘½ä»¤éƒ¨é˜Ÿä»¥é“ç‚®å°„å‡»[47]ã€‚ç„¶åè¿›å…¥æ··æˆ˜,å†è¿›è¡Œçªå‡»,è—¤å ‚é«˜åˆ‘ã€æ°èƒœå’Œæ¡‘åä¸€å­æˆ˜æ­»',
  'å®‰å¨œ:é•¿å®—æˆ‘éƒ¨ä¸‰å†›è¿›è¡ŒæŒŸå‡»æœ¬é˜Ÿ'],
 ['æ°å…‹:ä½ åœ¨å“ªé‡Œ? é•¿å®—æˆ‘éƒ¨ä¸‰å†›è¿›è¡ŒæŒŸå‡»æœ¬é˜Ÿ',
  'å®‰å¨œ:ä¸ºä»€ä¹ˆ?',
  'æ°å…‹:æ˜¯çš„,æˆ‘ä»¬åšå®ˆé•¿æ¿‘å·,å½“è—¤å ‚å†›æ¥è¿‘æ—¶,ä»–ä»¬ç«‹å³å‘½ä»¤éƒ¨é˜Ÿä»¥é“ç‚®å°„å‡»[47]ã€‚ç„¶åè¿›å…¥æ··æˆ˜',
  'å®‰å¨œ:è¿™æ˜¯å“ªé‡Œ?',
  'æ°å…‹:è—¤å ‚é«˜åˆ‘ã€æ°èƒœå’Œæ¡‘åä¸€å­åœ¨æˆ˜æ–—ä¸­æ­»äº¡ã€‚',
  'å®‰å¨œ:è¿™å¾ˆä»¤äººéœ‡æƒŠã€‚',
  'æ°å…‹:è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„é˜Ÿé•¿è¢«è§£æ•£åŸå› ã€‚',
  'å®‰å¨œ:é‚£æˆ‘ä»¬å°±ç­‰ç€å§ã€‚']]
```

#### Image Context Dialogue Generator

With the help of some excellent Image Caption models. (i.e.
[nlpconnect/vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning ) in English and [YeungNLP/ofa-cn-base-muge-v2](https://huggingface.co/YeungNLP/ofa-cn-base-muge-v2) in Chinese
   ), we can extend the context modal from text to image.

* 1 English Dialogue Generator in Image ğŸ¦…:

<div><img src='pic/black_man.jpeg' width="550" height="450" /></div>

</br>


```python
from summary_reverse_pred_eng_native import *

img_path = "pic/black_man.jpeg"

df = sample_pred_wrapper(img_path, i2c_obj = i2c_obj)
df["dialogue"].values.tolist()
```

will output:
```json
[['file_photo>', 'A man in black and white', 'Ok'],
 ['Man in a black and white photo', 'Good!', "I'm watching it now"],
 ['file_photo>', 'what is that?', 'man in black and white', 'ok'],
 ['file_photo>', 'a man in a black and white photo', "oh, that's what I saw"],
 ['file_photo>', 'This man in black and white', "He's the best!", 'I know.']]
```

</br>

* 2 Chinese Dialogue Generator in Image ğŸ°:

<div><img src='pic/cat.jpg' width="550" height="450" /></div>

</br>


```python
from summary_reverse_pred_native import *

img_path = "pic/cat.jpg"

df = sample_pred_wrapper(img_path, i2c_obj = ofa_obj)
df["dialogue"].values.tolist()
```

will output:
```json
[['æ°å…‹:ä½ è§è¿‡å¯çˆ±çš„çŒ«å’ªå—?', 'å®‰å¨œ:æ˜¯çš„,æˆ‘è§è¿‡ã€‚', 'æ°å…‹:å¤ªå¯çˆ±äº†!'],
 ['æ°å…‹:ä½ è§è¿‡å¯çˆ±çš„çŒ«å—?', 'å®‰å¨œ:æ˜¯çš„,æˆ‘è§è¿‡ã€‚', 'æ°å…‹:<file_gif>'],
 ['æ°å…‹:å˜¿,å®è´,ä½ è§è¿‡å¯çˆ±çš„çŒ«å’ªå—?', 'å®‰å¨œ:æ˜¯çš„,æˆ‘è§è¿‡ã€‚', 'æ°å…‹:å“¦,å¯¹äº†!', 'å®‰å¨œ:<file_gif>'],
 ['æ°å…‹:<file_photo>ã€‚', 'å®‰å¨œ:å¯çˆ±çš„çŒ«å’ª,ä½ è§è¿‡å—?', 'æ°å…‹:æ˜¯çš„,æˆ‘éå¸¸å–œæ¬¢å®ƒ!'],
 ['æ°å…‹:ä½ è§è¿‡å¯çˆ±çš„çŒ«å’ªå—?', 'å®‰å¨œ:æ˜¯çš„,å®ƒå¾ˆå¯çˆ±ã€‚', 'æ°å…‹:æˆ‘çœ‹äº†å®ƒä»¬çš„ç…§ç‰‡,ä»–ä»¬çœ‹èµ·æ¥éå¸¸å¯çˆ±ã€‚', 'å®‰å¨œ:è°¢!']]
```

</br>

</br>
Set "extend_by_diffusion" parameter to True will use prompt extend model that will make the caption more vivid in stable diffusion prompt format. </br>

Some details can be seen in 

[svjack/Stable-Diffusion-Chinese-Extend](https://github.com/svjack/Stable-Diffusion-Chinese-Extend)

</br>
</br>
Below is the the demonstration.

```python
from summary_reverse_pred_eng_native import *

img_path = "pic/black_man.jpeg"

df = sample_pred_wrapper(img_path, i2c_obj = i2c_obj, extend_by_diffusion = True)
df["dialogue"].values.tolist()
```

```json
[['what is this man wearing?',
  'black and white photo by robert mapplethorpe and paolo roversi',
  'file_photo>',
  'hahaha, the color is so vivid that itâ€™s hard to see how dark it really is',
  'yeah, thereâ€™s a lot of detail in this picture',
  'I havenâ€™t seen him for ages',
  'just look at his eyes',
  'he looks like heâ€™s having a good day'],
 ['file_photo>',
  "what's that?",
  'a man in a black and white photo style, a black and white photo by ryan church',
  'trending on shutterstock',
  'neo-periptivism',
  'movie still, movie poster, poster art national anthem 3',
  'concert poster, poster art toyism'],
 ['Look at this',
  'file_photo>',
  "What's that?",
  'Lee Jeffries outdoor, 35mm Pentax studio lighting, studio lighting, highly detailed, sharp focus, masterpiece, concept art, trending on artstation, head and shoulders shot, rule of thirds',
  'Wow!'],
 ['file_photo>',
  'What is that?',
  "You're talking about the black and white photo from 1 9 6 8's.",
  'Hahah, how d live in television ransacked at a computer in a basement.',
  "I don't know how to do it.",
  "It's not like you can do anything"],
 ['file_photo>',
  "OMG, that's amazing! I can't believe it's black and white. The colors are so close to the ground...",
  'Yeah, they look really cool in this photo. And what do you think about this?',
  "Well, there's some ultrafine detail, very chiaroscuro lighting, private press, association press photo # film, movie still Proviablize, concert poster, concert poster for the band â€œBack To The Shieldingâ€",
  'Oh, yeah, nice!']]
```

</br>

```python
from summary_reverse_pred_native import *

img_path = "pic/cat.jpg"

df = sample_pred_wrapper(img_path, i2c_obj = ofa_obj, extend_by_diffusion = True)
df["dialogue"].values.tolist()
```

</br>

will output:
```json
[['æ°å…‹:å˜¿,å®è´ä»¬,ä½ ä»¬çœ‹åˆ°å¯çˆ±çš„çŒ«å’ªäº†å—?',
  'å®‰å¨œ:æ˜¯çš„,å®ƒå¤ªå¯çˆ±äº†!',
  'æ°å…‹:ä½ çœ‹è¿‡ã€ŠçŒ«çš„è‰ºæœ¯ã€‹å—?',
  'å®‰å¨œ:æ˜¯çš„,å®ƒéå¸¸ç²¾å½©ã€‚',
  'æ°å…‹:è¿™æ˜¯æœ€ç²¾å½©çš„éƒ¨åˆ†ã€‚ 4k. é«˜åˆ†è¾¨ç‡',
  'å®‰å¨œ:é‚£æ˜¯ä»€ä¹ˆ?',
  'æ°å…‹:å®ƒä»¬åœ¨ä¸åŒçš„æ—¶é—´æœ‰é¢œè‰²å’Œç±»å‹ã€‚',
  'å®‰å¨œ:ä»–ä»¬çœ‹èµ·æ¥çœŸä¸é”™ã€‚',
  'æ°å…‹:è¿™çœŸæ˜¯å¤ªå®Œç¾äº†ã€‚',
  'å®‰å¨œ:<file_photo>ã€‚'],
 ['æ°å…‹:ä½ çœ‹åˆ°å¯çˆ±çš„çŒ«å’ªäº†å—?',
  'å®‰å¨œ:æ˜¯çš„,æˆ‘çœ‹åˆ°äº†ã€‚',
  'æ°å…‹:å¥¹ç©¿ç€é»‘è‰²çš„è¡£æœ,ç«™åœ¨æ£®æ—é‡Œã€‚',
  'å®‰å¨œ:æœ‰ä¸€å¤´ç™½è‰²çš„é•¿å‘å—?',
  'æ°å…‹:<file_photo>ã€‚'],
 ['æ°å…‹:ä½ çœ‹åˆ°å¯çˆ±çš„çŒ«å’ªäº†å—?',
  'å®‰å¨œ:æ˜¯çš„,æˆ‘çœ‹åˆ°äº†ã€‚',
  'æ°å…‹:<file_photo>ã€‚',
  'å®‰å¨œ:å®ƒæ˜¯å½©è‰²é“…ç¬”è‰ºæœ¯å—?',
  'æ°å…‹:æ˜¯çš„,å®ƒå¤ªå¯çˆ±äº†!',
  'å®‰å¨œ:å“¦,æˆ‘å–œæ¬¢å®ƒ!ã€‚',
  'æ°å…‹:çœŸçš„å—?',
  'å®‰å¨œ:æ˜¯çš„,å®ƒéå¸¸æœ‰è¶£!',
  'æ°å…‹:è¿™å¾ˆå®Œç¾!',
  'å®‰å¨œ:å®ƒä»¬å¾ˆæœ‰è¶£!',
  'æ°å…‹:å®ƒä»¬çš„å°ºå¯¸çœŸçš„å¾ˆå¤§,å¤§çº¦3.5å˜ç±³ã€‚',
  'å®‰å¨œ:çœ‹èµ·æ¥çœŸä¸é”™!',
  'æ°å…‹:ä½ èƒ½æƒ³è±¡å—?',
  'å®‰å¨œ:æˆ‘æƒ³è¿™æ˜¯æœ€æ˜‚è´µçš„ã€‚',
  'æ°å…‹:å—¯,ä¸å®Œå…¨æ˜¯ã€‚',
  'å®‰å¨œ:è°¢ä½ ,å®è´ä»¬!',
  'æ°å…‹:åœ¨artstationä¸Šä¹Ÿå¾ˆæµè¡Œã€‚'],
 ['æ°å…‹:ä½ è§è¿‡å¯çˆ±çš„çŒ«å’ªå—?',
  'å®‰å¨œ:å“¦,æˆ‘è§è¿‡ã€‚å·¥ä½œå®¤ç¯ç«é€šçº¢,ç°è‰²èƒŒæ™¯,å•ä¸€èº«ä½“,æ²¡æœ‰é˜´å½±,æ··åˆå™¨,åœ¨artstationä¸Šè¶‹åŠ¿,é«˜åº¦è¯¦ç»†)å½©è‰²',
  'æ°å…‹:ä½ å–œæ¬¢å®ƒä»¬å—?',
  'å®‰å¨œ:ä¸,å®ƒçœ‹èµ·æ¥åƒä¸€åªç‹—,ä½†æ˜¯å¦‚æ­¤å¯çˆ±ã€‚',
  'æ°å…‹:æ˜¯çš„,æˆ‘å–œæ¬¢å®ƒä»¬ã€‚'],
 ['æ°å…‹:å˜¿,ä½ çœ‹åˆ°å¯çˆ±çš„çŒ«å’ªäº†å—?',
  'å®‰å¨œ:æ˜¯çš„,æˆ‘çœ‹åˆ°äº†ã€‚',
  'æ°å…‹:ä½ çš„æ•°å­—è‰ºæœ¯æ˜¯ä»€ä¹ˆ?',
  'å®‰å¨œ:<file_photo>ã€‚',
  'æ°å…‹:å®ƒçœ‹èµ·æ¥çœŸä¸é”™!',
  'å®‰å¨œ:å®ƒæ˜¯å¦‚æ­¤çš„è¶…ç°å®ä¸»ä¹‰ã€‚',
  'æ°å…‹:é«˜ç»†èŠ‚ã€ã€‚',
  'å®‰å¨œ:å“‡å“¦!è¿™çœŸæ˜¯å¤ªç¥å¥‡äº†',
  'æ°å…‹:æˆ‘ä»¬ä¸€ç›´åœ¨è°ˆè®ºè¿™ä¸ªç³»åˆ—ã€‚',
  'å®‰å¨œ:æˆ‘æƒ³æŠŠå®ƒå˜æˆä¸€ä¸ªéå¸¸æœ‰è¶£çš„ä¸œè¥¿ã€‚',
  'æ°å…‹:ä½†å®ƒä¼¼ä¹å¾ˆæœ‰è¶£ã€‚',
  'å®‰å¨œ:ä½†å®ƒä»¬å¾ˆæ€§æ„Ÿã€‚',
  'æ°å…‹:å°±åƒçœŸæ­£çš„çŒ«å—?',
  'å®‰å¨œ:æ˜¯çš„ã€‚',
  'æ°å…‹:å®ƒçš„ä¸»é¢˜æ˜¯ä»€ä¹ˆ?',
  'å®‰å¨œ:æ–‡è‰ºå¤å…´é£æ ¼ã€‚',
  'æ°å…‹:å…¸å‹çš„ç°ä»£ä¸»ä¹‰ã€‚',
  'å®‰å¨œ:åƒä»€ä¹ˆ?']]
```

## More Info and Disscussion
Set "do_sample" to True can be a simple method to do augmentation on the outputs and prevent from get trivial outputs. One can do this multi times and find some good samples by setting simple rules.

### Self trained other Related Model demonstration 
|Name |HuggingFace Model link| Task | Language | Model skeleton |
|---------|--------|-------|-------|-------|
| svjack/dialogue-summary | https://huggingface.co/svjack/dialogue-summary | Generate summary of a dialogue context | Chinese | T5 |
| svjack/dialogue-summary-fill-characters | https://huggingface.co/svjack/dialogue-summary-fill-characters | Map dialogue character to summary position | Chinese | T5 |
| svjack/vit-gpt-diffusion-zh | https://huggingface.co/svjack/vit-gpt-diffusion-zh | Generate stable diffusion style caption of Image | Chinese | VIT X GPT2 |

<!-- CONTACT -->
## Contact

<!--
Your Name - [@your_twitter](https://twitter.com/your_username) - email@example.com
-->
svjack - https://huggingface.co/svjack - svjackbt@gmail.com - ehangzhou@outlook.com

<!--
Project Link: [https://github.com/your_username/repo_name](https://github.com/your_username/repo_name)
-->
Project Link:[https://github.com/svjack/Context2Dialogue](https://github.com/svjack/Context2Dialogue)


<!-- ACKNOWLEDGEMENTS -->
## Acknowledgements
<!--
* [GitHub Emoji Cheat Sheet](https://www.webpagefx.com/tools/emoji-cheat-sheet)
* [Img Shields](https://shields.io)
* [Choose an Open Source License](https://choosealicense.com)
* [GitHub Pages](https://pages.github.com)
* [Animate.css](https://daneden.github.io/animate.css)
* [Loaders.css](https://connoratherton.com/loaders)
* [Slick Carousel](https://kenwheeler.github.io/slick)
* [Smooth Scroll](https://github.com/cferdinandi/smooth-scroll)
* [Sticky Kit](http://leafo.net/sticky-kit)
* [JVectorMap](http://jvectormap.com)
* [Font Awesome](https://fontawesome.com)
* [Bigscience](https://bigscience.huggingface.co)
* [TextBox](https://github.com/RUCAIBox/TextBox)
* [Langboat](https://huggingface.co/Langboat)
* [uer](https://huggingface.co/uer)
-->
* [deepset-wikipedia-assistant](https://huggingface.co/spaces/deepset/wikipedia-assistant)
* [ClueAI](https://huggingface.co/ClueAI)
* [nlpconnect/vit-gpt2-image-captioning](https://huggingface.co/svjack/prompt-extend-chinese-gpt)
* [YeungNLP/ofa-cn-base-muge-v2](https://huggingface.co/YeungNLP/ofa-cn-base-muge-v2)
* [daspartho/prompt-extend](https://huggingface.co/daspartho/prompt-extend )
* [svjack/Daliy-Dialogue](https://github.com/svjack/Daliy-Dialogue)
* [svjack/GLM-Open-Dialogue](https://github.com/svjack/GLM-Open-Dialogue)
* [svjack/docvqa-gen](https://github.com/svjack/docvqa-gen)
* [svjack/Stable-Diffusion-Chinese-Extend](https://github.com/svjack/Stable-Diffusion-Chinese-Extend)
* [svjack](https://huggingface.co/svjack)
